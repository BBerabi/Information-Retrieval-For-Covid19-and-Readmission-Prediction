{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ander/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif, chi2 \n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    data_original = data.copy()\n",
    "    data = data.replace('?', np.NaN)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    # weights, payer_code, diag_1_desc, diag_2_desc, diag_3_desc\n",
    "    data.drop(labels=['weight', 'payer_code', 'diag_1_desc', 'diag_2_desc', 'diag_3_desc'], axis=1, inplace=True)\n",
    "\n",
    "    data['diag_1'] = group_diagnoses(data['diag_1'])\n",
    "    data['diag_2'] = group_diagnoses(data['diag_2'])\n",
    "    data['diag_3'] = group_diagnoses(data['diag_3'])\n",
    "\n",
    "    # Encode string data to numericals\n",
    "    to_cat = list(data.select_dtypes(['object']).columns)\n",
    "    data[to_cat] = data[to_cat].astype('category')\n",
    "    cat_columns = data.select_dtypes(['category']).columns\n",
    "    data[cat_columns] = data[cat_columns].apply(lambda x: x.cat.codes)\n",
    "\n",
    "    # Get Readmitted as labels\n",
    "    labels = data['readmitted']\n",
    "    data.drop(labels=['readmitted'], axis=1, inplace=True)\n",
    "    # data = data.replace(-1, np.NaN)\n",
    "\n",
    "\n",
    "    return labels.values.ravel(), data.values\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(text) \n",
    "    text = [w for w in word_tokens if not w in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return WordNetLemmatizer().lemmatize(text)\n",
    "\n",
    "\n",
    "def extract_textual_features(data, colname):\n",
    "    corpus = data[colname]\n",
    "    corpus = corpus.replace(np.NaN, '').values\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5, min_df=0.0001)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    text_features = pd.DataFrame.sparse.from_spmatrix(X)\n",
    "    return text_features\n",
    "\n",
    "\n",
    "def group_diagnoses(df):\n",
    "    # Create mapping from\n",
    "    l_old = []\n",
    "    l_new = []\n",
    "\n",
    "    idx = 0\n",
    "    tmp_list1 = list(range(390, 460))\n",
    "    tmp_list1 += [785]\n",
    "    tmp_list2 = [idx] * len(tmp_list1)\n",
    "    idx += 1\n",
    "\n",
    "    l_old = [*l_old, *tmp_list1]\n",
    "    l_new = [*l_new, *tmp_list2]\n",
    "\n",
    "    tmp_list1 = list(range(460, 520))\n",
    "    tmp_list1 += [786]\n",
    "    tmp_list2 = [idx] * len(tmp_list1)\n",
    "    idx += 1\n",
    "\n",
    "    l_old = [*l_old, *tmp_list1]\n",
    "    l_new = [*l_new, *tmp_list2]\n",
    "\n",
    "    tmp_list1 = list(range(520, 579))\n",
    "    tmp_list1 += [787]\n",
    "    tmp_list2 = [idx] * len(tmp_list1)\n",
    "    idx += 1\n",
    "\n",
    "    l_old = [*l_old, *tmp_list1]\n",
    "    l_new = [*l_new, *tmp_list2]\n",
    "\n",
    "    tmp_list1 = [str(i) for i in list(np.arange(250, 251, 0.01))]\n",
    "    tmp_list2 = [idx] * len(tmp_list1)\n",
    "    idx += 1\n",
    "    l_old = [*l_old, *tmp_list1]\n",
    "    l_new = [*l_new, *tmp_list2]\n",
    "\n",
    "    tmp_list1 = range(800, 1000)\n",
    "    tmp_list2 = [idx] * len(tmp_list1)\n",
    "    idx += 1\n",
    "    l_old = [*l_old, *tmp_list1]\n",
    "    l_new = [*l_new, *tmp_list2]\n",
    "\n",
    "    tmp_list1 = range(710, 740)\n",
    "    tmp_list2 = [idx] * len(tmp_list1)\n",
    "    idx += 1\n",
    "    l_old = [*l_old, *tmp_list1]\n",
    "    l_new = [*l_new, *tmp_list2]\n",
    "\n",
    "    tmp_list1 = list(range(580, 630))\n",
    "    tmp_list1 += [788]\n",
    "    tmp_list2 = [idx] * len(tmp_list1)\n",
    "    idx += 1\n",
    "    l_old = [*l_old, *tmp_list1]\n",
    "    l_new = [*l_new, *tmp_list2]\n",
    "\n",
    "    tmp_list1 = range(140, 240)\n",
    "    tmp_list2 = [idx] * len(tmp_list1)\n",
    "    idx += 1\n",
    "    l_old = [*l_old, *tmp_list1]\n",
    "    l_new = [*l_new, *tmp_list2]\n",
    "\n",
    "    l_old = [str(i) for i in l_old]\n",
    "    d = dict(zip(l_old, l_new))\n",
    "\n",
    "    df_new = df.copy()\n",
    "\n",
    "    df_new = df_new.map(d)\n",
    "    df_new = df_new.replace(df_new[pd.isna(df_new)], 8)\n",
    "    df_new = df_new.astype(int)\n",
    "    return df_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (10000, 45)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>weight</th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>payer_code</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>...</th>\n",
       "      <th>glipizide.metformin</th>\n",
       "      <th>glimepiride.pioglitazone</th>\n",
       "      <th>metformin.rosiglitazone</th>\n",
       "      <th>metformin.pioglitazone</th>\n",
       "      <th>change</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>readmitted</th>\n",
       "      <th>diag_1_desc</th>\n",
       "      <th>diag_2_desc</th>\n",
       "      <th>diag_3_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>Male</td>\n",
       "      <td>[60-70)</td>\n",
       "      <td>?</td>\n",
       "      <td>Emergency</td>\n",
       "      <td>Discharged to home</td>\n",
       "      <td>Emergency Room</td>\n",
       "      <td>4</td>\n",
       "      <td>MC</td>\n",
       "      <td>?</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Acute pericarditis in diseases classified else...</td>\n",
       "      <td>Secondary malignant neoplasm of kidney</td>\n",
       "      <td>Congestive heart failure, unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[70-80)</td>\n",
       "      <td>?</td>\n",
       "      <td>Elective</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Physician Referral</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>Family/GeneralPractice</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Malignant essential hypertension</td>\n",
       "      <td>Spinal stenosis, unspecified region</td>\n",
       "      <td>Diabetes mellitus without mention of complicat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[80-90)</td>\n",
       "      <td>?</td>\n",
       "      <td>Urgent</td>\n",
       "      <td>Discharged/transferred to SNF</td>\n",
       "      <td>Emergency Room</td>\n",
       "      <td>2</td>\n",
       "      <td>MC</td>\n",
       "      <td>Emergency/Trauma</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Urinary tract infection, site not specified</td>\n",
       "      <td>Streptococcus infection in conditions classifi...</td>\n",
       "      <td>Congestive heart failure, unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>Female</td>\n",
       "      <td>[50-60)</td>\n",
       "      <td>?</td>\n",
       "      <td>Emergency</td>\n",
       "      <td>Discharged to home</td>\n",
       "      <td>Emergency Room</td>\n",
       "      <td>4</td>\n",
       "      <td>DM</td>\n",
       "      <td>?</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Respiratory abnormality, unspecified</td>\n",
       "      <td>Hypertensive chronic kidney disease, malignant...</td>\n",
       "      <td>Diabetes mellitus without mention of complicat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[80-90)</td>\n",
       "      <td>?</td>\n",
       "      <td>Elective</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Physician Referral</td>\n",
       "      <td>13</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Coronary atherosclerosis of unspecified type o...</td>\n",
       "      <td>Chronic airway obstruction, not elsewhere clas...</td>\n",
       "      <td>Malignant essential hypertension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>?</td>\n",
       "      <td>Male</td>\n",
       "      <td>[40-50)</td>\n",
       "      <td>?</td>\n",
       "      <td>Urgent</td>\n",
       "      <td>Discharged to home</td>\n",
       "      <td>Transfer from a hospital</td>\n",
       "      <td>1</td>\n",
       "      <td>UN</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Coronary atherosclerosis of unspecified type o...</td>\n",
       "      <td>Diabetes mellitus without mention of complicat...</td>\n",
       "      <td>Pure hypercholesterolemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[80-90)</td>\n",
       "      <td>?</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Discharged/transferred to SNF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Atherosclerosis of aorta</td>\n",
       "      <td>Endomyocardial fibrosis</td>\n",
       "      <td>Diabetes mellitus without mention of complicat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>Male</td>\n",
       "      <td>[40-50)</td>\n",
       "      <td>?</td>\n",
       "      <td>Elective</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Physician Referral</td>\n",
       "      <td>4</td>\n",
       "      <td>?</td>\n",
       "      <td>Urology</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Malignant neoplasm of prostate</td>\n",
       "      <td>Hypertrophy (benign) of prostate without urina...</td>\n",
       "      <td>Obesity, unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>Male</td>\n",
       "      <td>[50-60)</td>\n",
       "      <td>?</td>\n",
       "      <td>Emergency</td>\n",
       "      <td>Discharged to home</td>\n",
       "      <td>Emergency Room</td>\n",
       "      <td>2</td>\n",
       "      <td>?</td>\n",
       "      <td>InternalMedicine</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Cellulitis and abscess of face</td>\n",
       "      <td>Diabetes mellitus without mention of complicat...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[80-90)</td>\n",
       "      <td>?</td>\n",
       "      <td>Elective</td>\n",
       "      <td>Discharged/transferred to another rehab fac in...</td>\n",
       "      <td>Physician Referral</td>\n",
       "      <td>1</td>\n",
       "      <td>CM</td>\n",
       "      <td>?</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Occlusion and stenosis of basilar artery witho...</td>\n",
       "      <td>Malignant essential hypertension</td>\n",
       "      <td>Diabetes mellitus without mention of complicat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 race  gender      age weight admission_type_id  \\\n",
       "0     AfricanAmerican    Male  [60-70)      ?         Emergency   \n",
       "1           Caucasian  Female  [70-80)      ?          Elective   \n",
       "2           Caucasian  Female  [80-90)      ?            Urgent   \n",
       "3     AfricanAmerican  Female  [50-60)      ?         Emergency   \n",
       "4           Caucasian    Male  [80-90)      ?          Elective   \n",
       "...               ...     ...      ...    ...               ...   \n",
       "9995                ?    Male  [40-50)      ?            Urgent   \n",
       "9996        Caucasian    Male  [80-90)      ?     Not Available   \n",
       "9997  AfricanAmerican    Male  [40-50)      ?          Elective   \n",
       "9998  AfricanAmerican    Male  [50-60)      ?         Emergency   \n",
       "9999        Caucasian  Female  [80-90)      ?          Elective   \n",
       "\n",
       "                               discharge_disposition_id  \\\n",
       "0                                    Discharged to home   \n",
       "1                                                   NaN   \n",
       "2                         Discharged/transferred to SNF   \n",
       "3                                    Discharged to home   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "9995                                 Discharged to home   \n",
       "9996                      Discharged/transferred to SNF   \n",
       "9997                                                NaN   \n",
       "9998                                 Discharged to home   \n",
       "9999  Discharged/transferred to another rehab fac in...   \n",
       "\n",
       "           admission_source_id  time_in_hospital payer_code  \\\n",
       "0               Emergency Room                 4         MC   \n",
       "1           Physician Referral                 1          ?   \n",
       "2               Emergency Room                 2         MC   \n",
       "3               Emergency Room                 4         DM   \n",
       "4           Physician Referral                13          ?   \n",
       "...                        ...               ...        ...   \n",
       "9995  Transfer from a hospital                 1         UN   \n",
       "9996                       NaN                 6          ?   \n",
       "9997        Physician Referral                 4          ?   \n",
       "9998            Emergency Room                 2          ?   \n",
       "9999        Physician Referral                 1         CM   \n",
       "\n",
       "           medical_specialty  ...  glipizide.metformin  \\\n",
       "0                          ?  ...                   No   \n",
       "1     Family/GeneralPractice  ...                   No   \n",
       "2           Emergency/Trauma  ...                   No   \n",
       "3                          ?  ...                   No   \n",
       "4                          ?  ...                   No   \n",
       "...                      ...  ...                  ...   \n",
       "9995              Cardiology  ...                   No   \n",
       "9996                       ?  ...                   No   \n",
       "9997                 Urology  ...                   No   \n",
       "9998        InternalMedicine  ...                   No   \n",
       "9999                       ?  ...                   No   \n",
       "\n",
       "      glimepiride.pioglitazone  metformin.rosiglitazone  \\\n",
       "0                           No                       No   \n",
       "1                           No                       No   \n",
       "2                           No                       No   \n",
       "3                           No                       No   \n",
       "4                           No                       No   \n",
       "...                        ...                      ...   \n",
       "9995                        No                       No   \n",
       "9996                        No                       No   \n",
       "9997                        No                       No   \n",
       "9998                        No                       No   \n",
       "9999                        No                       No   \n",
       "\n",
       "      metformin.pioglitazone  change  diabetesMed readmitted  \\\n",
       "0                         No      No           No          0   \n",
       "1                         No      Ch          Yes          1   \n",
       "2                         No      Ch          Yes          0   \n",
       "3                         No      Ch          Yes          1   \n",
       "4                         No      Ch          Yes          1   \n",
       "...                      ...     ...          ...        ...   \n",
       "9995                      No      No          Yes          0   \n",
       "9996                      No      No          Yes          0   \n",
       "9997                      No      No          Yes          0   \n",
       "9998                      No      No          Yes          0   \n",
       "9999                      No      No           No          0   \n",
       "\n",
       "                                            diag_1_desc  \\\n",
       "0     Acute pericarditis in diseases classified else...   \n",
       "1                      Malignant essential hypertension   \n",
       "2           Urinary tract infection, site not specified   \n",
       "3                  Respiratory abnormality, unspecified   \n",
       "4     Coronary atherosclerosis of unspecified type o...   \n",
       "...                                                 ...   \n",
       "9995  Coronary atherosclerosis of unspecified type o...   \n",
       "9996                           Atherosclerosis of aorta   \n",
       "9997                     Malignant neoplasm of prostate   \n",
       "9998                     Cellulitis and abscess of face   \n",
       "9999  Occlusion and stenosis of basilar artery witho...   \n",
       "\n",
       "                                            diag_2_desc  \\\n",
       "0                Secondary malignant neoplasm of kidney   \n",
       "1                   Spinal stenosis, unspecified region   \n",
       "2     Streptococcus infection in conditions classifi...   \n",
       "3     Hypertensive chronic kidney disease, malignant...   \n",
       "4     Chronic airway obstruction, not elsewhere clas...   \n",
       "...                                                 ...   \n",
       "9995  Diabetes mellitus without mention of complicat...   \n",
       "9996                            Endomyocardial fibrosis   \n",
       "9997  Hypertrophy (benign) of prostate without urina...   \n",
       "9998  Diabetes mellitus without mention of complicat...   \n",
       "9999                   Malignant essential hypertension   \n",
       "\n",
       "                                            diag_3_desc  \n",
       "0                 Congestive heart failure, unspecified  \n",
       "1     Diabetes mellitus without mention of complicat...  \n",
       "2                 Congestive heart failure, unspecified  \n",
       "3     Diabetes mellitus without mention of complicat...  \n",
       "4                      Malignant essential hypertension  \n",
       "...                                                 ...  \n",
       "9995                          Pure hypercholesterolemia  \n",
       "9996  Diabetes mellitus without mention of complicat...  \n",
       "9997                               Obesity, unspecified  \n",
       "9998                                                NaN  \n",
       "9999  Diabetes mellitus without mention of complicat...  \n",
       "\n",
       "[10000 rows x 51 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('task1/data/diab_train.csv', index_col=0)\n",
    "data_test = pd.read_csv('task1/data/diab_test.csv', index_col=0)\n",
    "data_validation = pd.read_csv('task1/data/diab_validation.csv', index_col=0)\n",
    "\n",
    "data_train = pd.concat([data_train, data_validation, data_test], axis=0)\n",
    "data_train.reset_index(drop=True, inplace=True)\n",
    "data = data_train.copy()\n",
    "y, X = preprocess_data(data_train)\n",
    "print(y.shape, X.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a function for xgboost training.\n",
    "#### We will call this function later for different types of textual feature extraction\n",
    "\n",
    "def train_xgboost(X, y):\n",
    "    X_train = X[:8000]\n",
    "    y_train = y[:8000]\n",
    "    y_test, X_test = y[8000:], X[8000:]\n",
    "    print('X_train shape: {}  and X_test shape: {}'.format(X_train.shape, X_test.shape))\n",
    "\n",
    "    scoring ={'auroc':    make_scorer(roc_auc_score, greater_is_better=True),\n",
    "            'f1_score': make_scorer(f1_score, average='micro', greater_is_better=True)}\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(scale_pos_weight=2, disable_defeult_eval_metric=0)\n",
    "\n",
    "    parameters = {\n",
    "        'objective': ['binary:logistic'],\n",
    "        'max_depth': [200],\n",
    "        'min_child_weight': [9],\n",
    "        'n_estimators': [500],\n",
    "        'seed': [11],\n",
    "        'learning_rate': [0.01],\n",
    "        'max_delta_step': [0],\n",
    "        'subsample': [0.75]\n",
    "    }\n",
    "\n",
    "    clf = GridSearchCV(estimator=xgb_model, param_grid=parameters, n_jobs=3, cv=4, scoring=scoring, verbose=3,\n",
    "                       refit='f1_score')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print('best score: ', clf.best_score_)\n",
    "    print('best parameters: ', clf.best_params_)\n",
    "\n",
    "    y_predict = clf.predict(X_test)\n",
    "    print(\"0 predictions: {}  1 predictions: {}\".format(np.count_nonzero(y_predict == 0) ,np.count_nonzero(y_predict == 1)))\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    aucroc_score = roc_auc_score(y_test, y_predict)\n",
    "    f1Score = f1_score(y_test, y_predict)\n",
    "    print('Accuracy: {} AUCROC: {} F1: {}'.format(accuracy, aucroc_score, f1Score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### There are 2 options: Either you can train all the model with features extracted only from primary diagnoses\n",
    "##### or you can train all the models with features extracted from all 3 description of diagnoses\n",
    "##### Change the OPTION1 variable appropriately. If you want features from all, set it to False\n",
    "\n",
    "OPTION1 = False\n",
    "\n",
    "if OPTION1:\n",
    "    textdata = data['diag_1_desc']\n",
    "    textdata = diag1.replace(np.NaN, '').values\n",
    "else:\n",
    "    textdata = pd.concat([data['diag_1_desc'], data['diag_2_desc'], data['diag_3_desc']], axis=0)\n",
    "    textdata.reset_index(inplace=True, drop=True)\n",
    "    textdata = textdata.replace(np.NaN, ' ').values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model (without NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8000, 45)  and X_test shape: (2000, 45)\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   4 out of   4 | elapsed:   18.0s finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-eb6f11a607be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_xgboost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-8db5f32601cc>\u001b[0m in \u001b[0;36mtrain_xgboost\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     26\u001b[0m     clf = GridSearchCV(estimator=xgb_model, param_grid=parameters, n_jobs=3, cv=4, scoring=scoring, verbose=3,\n\u001b[1;32m     27\u001b[0m                        refit='f1_score')\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    821\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    207\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1247\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[1;32m   1248\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_xgboost(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berka\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(10000, 3552)\n",
      "1184\n",
      "(10000, 1184) (10000, 1184) (10000, 1184)\n",
      "(10000, 65)\n",
      "X_train shape: (8000, 65)  and X_test shape: (2000, 65)\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   4 out of   4 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score:  0.62375\n",
      "best parameters:  {'learning_rate': 0.01, 'max_delta_step': 0, 'max_depth': 200, 'min_child_weight': 9, 'n_estimators': 500, 'objective': 'binary:logistic', 'seed': 11, 'subsample': 0.75}\n",
      "0 predictions: 988  1 predictions: 1012\n",
      "Accuracy: 0.6375 AUCROC: 0.6449530951751605 F1: 0.5983379501385041\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.5, preprocessor=preprocess_text)\n",
    "text_features = vectorizer.fit_transform(textdata)\n",
    "text_features = pd.DataFrame.sparse.from_spmatrix(text_features).values\n",
    "\n",
    "print(text_features)\n",
    "\n",
    "text_features = text_features.reshape(10000,-1)\n",
    "print(text_features.shape)\n",
    "\n",
    "if OPTION1==False:\n",
    "    dim = int(text_features.shape[1]/3)\n",
    "    print(dim)\n",
    "    diag1 = text_features[:,:dim]\n",
    "    diag2 = text_features[:,dim:dim*2]\n",
    "    diag3 = text_features[:, 2*dim:]\n",
    "    print(diag1.shape, diag2.shape, diag3.shape)\n",
    "\n",
    "    text_features = diag1 + diag2 + diag3 \n",
    "    text_features.shape\n",
    "\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=20)\n",
    "text_features = selector.fit_transform(text_features, y)\n",
    "text_features.shape\n",
    "scaler = StandardScaler()\n",
    "text_features = scaler.fit_transform(text_features)\n",
    "\n",
    "# now add textual features to the categorical features\n",
    "Xnew = np.concatenate((X, text_features), axis=1)\n",
    "print(Xnew.shape)\n",
    "\n",
    "train_xgboost(Xnew, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berka\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(10000, 3552)\n",
      "(10000, 65)\n",
      "X_train shape: (8000, 65)  and X_test shape: (2000, 65)\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   4 out of   4 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score:  0.624\n",
      "best parameters:  {'learning_rate': 0.01, 'max_delta_step': 0, 'max_depth': 200, 'min_child_weight': 9, 'n_estimators': 500, 'objective': 'binary:logistic', 'seed': 11, 'subsample': 0.75}\n",
      "0 predictions: 973  1 predictions: 1027\n",
      "Accuracy: 0.643 AUCROC: 0.6523213160723857 F1: 0.6076923076923076\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5, preprocessor=preprocess_text)\n",
    "text_features = vectorizer.fit_transform(textdata)\n",
    "text_features = pd.DataFrame.sparse.from_spmatrix(text_features).values\n",
    "\n",
    "print(text_features)\n",
    "\n",
    "text_features = text_features.reshape(10000,-1)\n",
    "print(text_features.shape)\n",
    "\n",
    "if OPTION1:\n",
    "    dim = int(text_features.shape[1]/3)\n",
    "    print(dim)\n",
    "    diag1 = text_features[:,:dim]\n",
    "    diag2 = text_features[:,dim:dim*2]\n",
    "    diag3 = text_features[:, 2*dim:]\n",
    "    print(diag1.shape, diag2.shape, diag3.shape)\n",
    "\n",
    "    text_features = diag1 + diag2 + diag3 \n",
    "    text_features.shape\n",
    "\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=20)\n",
    "text_features = selector.fit_transform(text_features, y)\n",
    "text_features.shape\n",
    "scaler = StandardScaler()\n",
    "text_features = scaler.fit_transform(text_features)\n",
    "\n",
    "# now add textual features to the categorical features\n",
    "Xnew = np.concatenate((X, text_features), axis=1)\n",
    "print(Xnew.shape)\n",
    "\n",
    "train_xgboost(Xnew, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berka\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 65)\n",
      "X_train shape: (8000, 65)  and X_test shape: (2000, 65)\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   4 out of   4 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score:  0.625125\n",
      "best parameters:  {'learning_rate': 0.01, 'max_delta_step': 0, 'max_depth': 200, 'min_child_weight': 9, 'n_estimators': 500, 'objective': 'binary:logistic', 'seed': 11, 'subsample': 0.75}\n",
      "0 predictions: 966  1 predictions: 1034\n",
      "Accuracy: 0.6275 AUCROC: 0.6368843578494929 F1: 0.5922276956759716\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.5, preprocessor=preprocess_text, ngram_range=(2,2))\n",
    "text_features = vectorizer.fit_transform(textdata)\n",
    "text_features = pd.DataFrame.sparse.from_spmatrix(text_features).values\n",
    "# print(vectorizer.get_feature_names())\n",
    "# print(text_features)\n",
    "\n",
    "text_features = text_features.reshape(10000,-1)\n",
    "# print(text_features.shape)\n",
    "\n",
    "if OPTION1:\n",
    "    dim = int(text_features.shape[1]/3)\n",
    "    diag1 = text_features[:,:dim]\n",
    "    diag2 = text_features[:,dim:dim*2]\n",
    "    diag3 = text_features[:, 2*dim:]\n",
    "    # print(diag1.shape, diag2.shape, diag3.shape)\n",
    "\n",
    "    text_features = diag1 + diag2 + diag3 \n",
    "    text_features.shape\n",
    "\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=20)\n",
    "text_features = selector.fit_transform(text_features, y)\n",
    "text_features.shape\n",
    "scaler = StandardScaler()\n",
    "text_features = scaler.fit_transform(text_features)\n",
    "\n",
    "# now add textual features to the categorical features\n",
    "Xnew = np.concatenate((X, text_features), axis=1)\n",
    "print(Xnew.shape)\n",
    "\n",
    "train_xgboost(Xnew, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berka\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 65)\n",
      "X_train shape: (8000, 65)  and X_test shape: (2000, 65)\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   4 out of   4 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score:  0.624125\n",
      "best parameters:  {'learning_rate': 0.01, 'max_delta_step': 0, 'max_depth': 200, 'min_child_weight': 9, 'n_estimators': 500, 'objective': 'binary:logistic', 'seed': 11, 'subsample': 0.75}\n",
      "0 predictions: 959  1 predictions: 1041\n",
      "Accuracy: 0.638 AUCROC: 0.6486113476348037 F1: 0.6052344601962922\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.5, preprocessor=preprocess_text, ngram_range=(3,3))\n",
    "text_features = vectorizer.fit_transform(textdata)\n",
    "text_features = pd.DataFrame.sparse.from_spmatrix(text_features).values\n",
    "# print(vectorizer.get_feature_names())\n",
    "# print(text_features)\n",
    "\n",
    "text_features = text_features.reshape(10000,-1)\n",
    "# print(text_features.shape)\n",
    "\n",
    "if OPTION1:\n",
    "    dim = int(text_features.shape[1]/3)\n",
    "    diag1 = text_features[:,:dim]\n",
    "    diag2 = text_features[:,dim:dim*2]\n",
    "    diag3 = text_features[:, 2*dim:]\n",
    "    #   print(diag1.shape, diag2.shape, diag3.shape)\n",
    "\n",
    "    text_features = diag1 + diag2 + diag3 \n",
    "    text_features.shape\n",
    "\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=20)\n",
    "text_features = selector.fit_transform(text_features, y)\n",
    "text_features.shape\n",
    "scaler = StandardScaler()\n",
    "text_features = scaler.fit_transform(text_features)\n",
    "\n",
    "# now add textual features to the categorical features\n",
    "Xnew = np.concatenate((X, text_features), axis=1)\n",
    "print(Xnew.shape)\n",
    "\n",
    "train_xgboost(Xnew, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram as TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berka\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4383)\n",
      "(10000, 65)\n",
      "X_train shape: (8000, 65)  and X_test shape: (2000, 65)\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   4 out of   4 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score:  0.6251249999999999\n",
      "best parameters:  {'learning_rate': 0.01, 'max_delta_step': 0, 'max_depth': 200, 'min_child_weight': 9, 'n_estimators': 500, 'objective': 'binary:logistic', 'seed': 11, 'subsample': 0.75}\n",
      "0 predictions: 967  1 predictions: 1033\n",
      "Accuracy: 0.638 AUCROC: 0.6477462803674655 F1: 0.6035049288061337\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5, preprocessor=preprocess_text, ngram_range=(2,2))\n",
    "text_features = vectorizer.fit_transform(textdata)\n",
    "text_features = pd.DataFrame.sparse.from_spmatrix(text_features).values\n",
    "# print(vectorizer.get_feature_names())\n",
    "# print(text_features)\n",
    "\n",
    "text_features = text_features.reshape(10000,-1)\n",
    "print(text_features.shape)\n",
    "\n",
    "if OPTION1:\n",
    "    dim = int(text_features.shape[1]/3)\n",
    "    diag1 = text_features[:,:dim]\n",
    "    diag2 = text_features[:,dim:dim*2]\n",
    "    diag3 = text_features[:, 2*dim:]\n",
    "    # print(diag1.shape, diag2.shape, diag3.shape)\n",
    "\n",
    "    text_features = diag1 + diag2 + diag3 \n",
    "    text_features.shape\n",
    "\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=20)\n",
    "text_features = selector.fit_transform(text_features, y)\n",
    "text_features.shape\n",
    "scaler = StandardScaler()\n",
    "text_features = scaler.fit_transform(text_features)\n",
    "\n",
    "# now add textual features to the categorical features\n",
    "Xnew = np.concatenate((X, text_features), axis=1)\n",
    "print(Xnew.shape)\n",
    "\n",
    "train_xgboost(Xnew, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram as TDIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berka\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 65)\n",
      "X_train shape: (8000, 65)  and X_test shape: (2000, 65)\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   4 out of   4 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score:  0.622125\n",
      "best parameters:  {'learning_rate': 0.01, 'max_delta_step': 0, 'max_depth': 200, 'min_child_weight': 9, 'n_estimators': 500, 'objective': 'binary:logistic', 'seed': 11, 'subsample': 0.75}\n",
      "0 predictions: 977  1 predictions: 1023\n",
      "Accuracy: 0.637 AUCROC: 0.6456201790522081 F1: 0.6002202643171806\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5, preprocessor=preprocess_text, ngram_range=(3,3))\n",
    "text_features = vectorizer.fit_transform(textdata)\n",
    "text_features = pd.DataFrame.sparse.from_spmatrix(text_features).values\n",
    "# print(vectorizer.get_feature_names())\n",
    "# print(text_features)\n",
    "\n",
    "text_features = text_features.reshape(10000,-1)\n",
    "# print(text_features.shape)\n",
    "\n",
    "if OPTION1:\n",
    "    dim = int(text_features.shape[1]/3)\n",
    "    diag1 = text_features[:,:dim]\n",
    "    diag2 = text_features[:,dim:dim*2]\n",
    "    diag3 = text_features[:, 2*dim:]\n",
    "#     print(diag1.shape, diag2.shape, diag3.shape)\n",
    "\n",
    "    text_features = diag1 + diag2 + diag3 \n",
    "    text_features.shape\n",
    "\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=20)\n",
    "text_features = selector.fit_transform(text_features, y)\n",
    "text_features.shape\n",
    "scaler = StandardScaler()\n",
    "text_features = scaler.fit_transform(text_features)\n",
    "\n",
    "# now add textual features to the categorical features\n",
    "Xnew = np.concatenate((X, text_features), axis=1)\n",
    "print(Xnew.shape)\n",
    "\n",
    "train_xgboost(Xnew, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentences = [word_tokenize(sentence) for sentence in textdata]\n",
    "model= Word2Vec(sentences, min_count=1, size=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8000, 95)  and X_test shape: (2000, 95)\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   4 out of   4 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score:  0.620125\n",
      "best parameters:  {'learning_rate': 0.01, 'max_delta_step': 0, 'max_depth': 200, 'min_child_weight': 9, 'n_estimators': 500, 'objective': 'binary:logistic', 'seed': 11, 'subsample': 0.75}\n",
      "0 predictions: 1107  1 predictions: 893\n",
      "Accuracy: 0.637 AUCROC: 0.6315628359579626 F1: 0.5693950177935944\n"
     ]
    }
   ],
   "source": [
    "def compute_features(sentence):\n",
    "    total = 0\n",
    "    for word in sentence:\n",
    "        total += model.wv[word]\n",
    "        \n",
    "    if len(sentence) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return total/len(sentence)\n",
    "\n",
    "text_features = [compute_features(sentence) for sentence in sentences]\n",
    "text_features = np.array(text_features)\n",
    "text_features.shape\n",
    "text_features = text_features.reshape(10000,-1)\n",
    "text_features = np.sum(text_features, axis=1)\n",
    "text_features = np.stack(text_features)\n",
    "text_features.shape\n",
    "\n",
    "scaler = StandardScaler()\n",
    "text_features = scaler.fit_transform(text_features)\n",
    "\n",
    "Xnew = np.concatenate((X, text_features), axis=1)\n",
    "\n",
    "train_xgboost(Xnew, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
