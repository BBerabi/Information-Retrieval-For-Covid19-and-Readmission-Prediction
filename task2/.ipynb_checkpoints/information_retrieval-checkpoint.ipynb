{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - CORD 19 Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we fetch the papers published in 2019 or 2020, create the correspondent BoW vectors for each of the papers and find relevant sentences according to a query from those papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare a csv file containing only papers published in 2019 and 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__IMPORTANT:__ We provide you this particular csv file, therefore you do not need to run the following script.If you do not want to wait for a long time, please skip this step! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "# Removes all cite spans and ref spans including the brackets\n",
    "# INPUT: json_file['body_text'][i]\n",
    "# OUTPUT: plain text in a string without any references or annotations\n",
    "def removeCitsRefs(json_paragraph):\n",
    "    ranges = []\n",
    "    places = []\n",
    "\n",
    "    places.extend(json_paragraph['cite_spans'])\n",
    "    places.extend(json_paragraph['ref_spans'])\n",
    "\n",
    "    # Get ranges to delete\n",
    "    for d in places:\n",
    "        tmp = (d['start'], d['end'])\n",
    "        ranges.append(tmp)\n",
    "    text_list = list(json_paragraph['text'])\n",
    "    for plc in ranges:\n",
    "        for i in range(plc[0], plc[1]):\n",
    "            text_list[i] = \" \"\n",
    "    text = ''.join(text_list)\n",
    "    return text\n",
    "\n",
    "\n",
    "# INPUT: json_file['body_text']\n",
    "# OUTPUT: Plain Text string\n",
    "def getText(json_body):\n",
    "    main_txt = \"\"\n",
    "    for i in range(len(json_body)):\n",
    "        tmp_txt = removeCitsRefs(json_body[i])\n",
    "        tmp_txt += \"\\n\"\n",
    "        main_txt += tmp_txt\n",
    "    return main_txt\n",
    "\n",
    "\n",
    "# PaperId, Title + Body,\n",
    "def getFileText(json_file):\n",
    "    body = json_file['metadata']['title'] + \" \\n\\n\" + getText(json_file['body_text'])\n",
    "    paper_id = json_file['paper_id']\n",
    "\n",
    "    return [paper_id, body]\n",
    "\n",
    "\n",
    "def createDataFrame(json_paths):\n",
    "    files = []\n",
    "    for j in range(len(json_paths)):\n",
    "        # load json\n",
    "        with open(json_paths[j], 'r') as f:\n",
    "            article_json = json.load(f)\n",
    "        files.append(getFileText(article_json))\n",
    "    df = pd.DataFrame(files, columns=['paper_id', 'text'])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Get data\n",
    "# metadata = pd.read_csv(\"./data/metadata.csv\", low_memory=False)\n",
    "path_csv = \"./data/papers19-20.csv\"\n",
    "path_metadata = './data/metadata.csv'\n",
    "\n",
    "#Get metadata\n",
    "metadata = pd.read_csv(path_metadata,low_memory=False)\n",
    "#Drop all rows that doesn't have sha identifier and publish time\n",
    "to_drop = list(metadata[pd.isna(metadata['publish_time'])].index)\n",
    "to_drop = to_drop + list(metadata[pd.isna(metadata['sha'])].index)\n",
    "to_drop = np.array(to_drop)\n",
    "to_drop = list(np.unique(to_drop))\n",
    "metadata = metadata.drop(to_drop,axis=0)\n",
    "print(\"Got metadata\")\n",
    "\n",
    "#Filter only rows with publish_time either 2019 or 2020\n",
    "p19 = metadata[metadata['publish_time'].str.contains('2019',regex=False)]\n",
    "p20 = metadata[metadata['publish_time'].str.contains('2020',regex=False)]\n",
    "metadata_papers = pd.concat([p19,p20],axis=0)\n",
    "print(\"Got metadata from 2019 and 2020\")\n",
    "\n",
    "files = pd.DataFrame(columns=[\"name\", \"path\"])\n",
    "\n",
    "#Get JSON Files' paths\n",
    "for dirname, _, filenames in os.walk('./data'):\n",
    "    for filename in filenames:\n",
    "        files = files.append({'name': filename, 'path': os.path.join(dirname, filename)}, ignore_index=True)\n",
    "print(\"Got file paths\")\n",
    "\n",
    "#Drop irrelevant files that are not json\n",
    "idx = files[files.name.str.contains(\".json\") == False].index\n",
    "files.drop(index=idx, inplace=True)\n",
    "files.reset_index(inplace=True)\n",
    "files.drop(\"index\", axis=1, inplace=True)\n",
    "print(\"Got all file paths\")\n",
    "#Erase extensions\n",
    "def getName(string):\n",
    "    for i in range(len(string)):\n",
    "        if string[i] == \".\":\n",
    "            return string[:i]\n",
    "files['file_name'] = files['name'].map(lambda file_name: getName(file_name))\n",
    "print(\"Delete extensions\")\n",
    "\n",
    "# Mark papers from 2019 or 2020\n",
    "files['sha_flag'] = files['file_name'].isin(list(metadata_papers['sha']))\n",
    "path_files = list(files[files['sha_flag']]['path'])\n",
    "\n",
    "\n",
    "print(\"Cleaned paths without .json extension\")\n",
    "df_text = createDataFrame(path_files)\n",
    "print(\"Created Dataframe\")\n",
    "\n",
    "df_text.to_csv(path_csv,index=False)\n",
    "print(\"Saved DataFrame to \",os.path.abspath(path_csv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Compute BoW vectors of the papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__IMPORTANT:__ Since it takes a lot of time to compute the BoW vectors we provide you the BoW vectors. If you do not want to wait for a lot of time please skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "import os\n",
    "import pickle\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the paths to get the relevant objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "PATH_CSV = os.path.abspath(\"./data/papers19-20.csv\")\n",
    "PATH_INDEX = './data/similarities.index'\n",
    "PATH_DICTIONARY = './data/dictionary19-20.dict'\n",
    "PATH_TFIDF = './data/tfidf.pkl'\n",
    "PATH_PICKLE = './data/bow_corpus19-20.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the papers data, tokenize them, find pos-tags of the words otherwise the lemmatization does not work and afterwards lemmatize them. \n",
    "Filter out the stop words and punctuation and also words that only appears once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(PATH_CSV)\n",
    "print(\"Got csv\")\n",
    "papers = data['text'].values\n",
    "\n",
    "# Tokenize\n",
    "punctuation = \",.?!()-_\\\"\\'\\\\\\n\\r\\t;:+*<>@#ยง^$%&|/\"\n",
    "processed = [[w.lower() for w in word_tokenize(document)] for document in papers]\n",
    "print(\"Tokenized\")\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "processed = [[lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in doc] for doc in processed]\n",
    "print(\"Lemmatized\")\n",
    "# Filter\n",
    "processed = [[w for w in doc if (w not in stopwords.words('english')) and (w not in punctuation)] for doc in processed]\n",
    "print(\"Filtered\")\n",
    "\n",
    "# Compute frequency\n",
    "frequency = defaultdict(int)\n",
    "for document in processed:\n",
    "    for token in document:\n",
    "        frequency[token] += 1\n",
    "print(\"Frequencies computed\")\n",
    "# Get only words with frequency >1\n",
    "processed_corpus = [[w for w in document if frequency[w] > 1] for document in processed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dictionary and get the BoW vectors. In case anything happens, save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it into dictionary\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "dictionary.save(PATH_DICTIONARY)\n",
    "# Create BoW vectors\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "print(\"BoW vectors are created\")\n",
    "\n",
    "with open(PATH_PICKLE,\"wb\") as f :\n",
    "    pickle.dump(bow_corpus,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Find the relevant sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from gensim import corpora, similarities\n",
    "from gensim import models\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for computing the bow_vectors of the relevant papers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBowVectors(papers,dictionary):\n",
    "    # Tokenize\n",
    "    punctuation = \",.?!()-_\\\"\\'\\\\\\n\\r\\t;:+*<>@#ยง^$%&|/\"\n",
    "    processed = [[w.lower() for w in word_tokenize(document)] for document in papers]\n",
    "\n",
    "\n",
    "    def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed = [[lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in doc] for doc in processed]\n",
    "    # Filter\n",
    "    processed = [[w for w in doc if (w not in stopwords.words('english')) and (w not in punctuation)] for doc in processed]\n",
    "    # Compute frequency\n",
    "    frequency = defaultdict(int)\n",
    "    for document in processed:\n",
    "        for token in document:\n",
    "            frequency[token] += 1\n",
    "    # Get only words with frequency >1\n",
    "    processed = [[w for w in document if frequency[w] > 1] for document in processed]\n",
    "    bow_vectors = [dictionary.doc2bow(text) for text in processed]\n",
    "    return bow_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the paths and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Number of relevant papers to fetch from corpus\n",
    "n_papers = 10\n",
    "n_sentences = 10\n",
    "#Get data\n",
    "PATH_CSV = os.path.abspath(\"./data/papers19-20.csv\")\n",
    "PATH_INDEX = './data/similarities.index'\n",
    "PATH_DICTIONARY = './data/dictionary19-20.dict'\n",
    "PATH_TFIDF = './data/tfidf.pkl'\n",
    "PATH_SENTENCES = './data/query_results.csv'\n",
    "PATH_PICKLE = './data/bow_corpus19-20.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the corpus' bow_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_PICKLE,\"rb\") as file :\n",
    "    bow_corpus = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get papers csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(PATH_CSV)\n",
    "print(\"Got csv\")\n",
    "papers = data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dictionary and create the tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dictionary\n",
    "dictionary = Dictionary.load(PATH_DICTIONARY)\n",
    "print(\"Loaded dictionary\")\n",
    "tfidf = models.TfidfModel(bow_corpus, id2word=dictionary, normalize=True, slope=0.25)\n",
    "# Similarity Index\n",
    "# Use especially similarity but not matrix similarity, since\n",
    "index = similarities.Similarity(None, tfidf[bow_corpus], num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__QUERY__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"risk factor corona virus 2019\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_query(query):\n",
    "    query = dictionary.doc2bow(query.lower().split(\" \"))\n",
    "    query = tfidf[query]\n",
    "    return query\n",
    "bow_query = process_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get documents similar to the query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get similar docs and sort them according to their value in descending order\n",
    "similars = index[bow_query]\n",
    "similars = sorted(enumerate(similars), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each paper find the keywords and the sentence they occured. Save them in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([],columns=['paper_id','sentence'])\n",
    "\n",
    "#look deeper in each paper\n",
    "for i in range(len(bow_papers)):\n",
    "    sentences = []\n",
    "    #Extract most important keywords of the document\n",
    "    keyword_ids = [ s[0] for s in sorted(tfidf[bow_papers[i]],key=lambda tup: -tup[-1])][:n_sentences]\n",
    "\n",
    "    keywords = []\n",
    "    for j in keyword_ids:\n",
    "        keywords.append(dictionary[j])\n",
    "\n",
    "    # Get paper and split it into sentences\n",
    "    tmp_paper_id = relevant_papers.iloc[i,0]\n",
    "    tmp_paper = relevant_papers.iloc[i,1]\n",
    "    tmp_paper = tmp_paper.split('.')\n",
    "    #Find the sentences containing those words\n",
    "    for sent in tmp_paper:\n",
    "        for k in keywords:\n",
    "            if k in sent:\n",
    "                sentences.append([tmp_paper_id,sent])\n",
    "                break\n",
    "\n",
    "    df = df.append(pd.DataFrame(sentences,columns=['paper_id','sentence']),ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)\n",
    "df.to_csv(PATH_SENTENCES)\n",
    "print(\"Relevant sentences are saved into \",PATH_SENTENCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
