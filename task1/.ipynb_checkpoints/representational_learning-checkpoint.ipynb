{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "text = ['the cat with the hat sat in the mat',\n",
    "        'you have brains in your head ',' you have feet in your shoes',\n",
    "        'You can steer yourself any direction you choose' ,\n",
    "        'Think and wonder, wonder and think', 'The more that you read',' the more things you will know',\n",
    "        'The more that you learn' ,'the more places you’ll go',\n",
    "        'the mat has a cat with a hat']\n",
    "\n",
    "txt = pd.DataFrame(text,columns=['text'])\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "filtered = []\n",
    "for w in word_tokenize(txt.text.values[0]):\n",
    "    if w in stopwords.words('english'):\n",
    "        continue\n",
    "    filtered.append(w)\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print([stemmer.stem(w) for w in word_tokenize(txt.text.values[3])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts/Occurences\n",
    "\n",
    "The model is simple in that it throws away all of the order information in the words and focuses on the occurrence of words in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_transformed = count_vect.fit_transform(txt.text.values)\n",
    "X_transformed.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectdf =pd.DataFrame(X_transformed.A.T)\n",
    "vectdf['term']=count_vect.get_feature_names()\n",
    "vectdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequencies\n",
    "\n",
    "Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
    "\n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf(corpus, stop_words=None):\n",
    "    tfidf_vect = TfidfVectorizer() if stop_words is None else TfidfVectorizer(stop_words=stop_words)\n",
    "    X_tf_transformed = tfidf_vect.fit_transform(corpus)\n",
    "    tfidf_vectdf = pd.DataFrame(X_tf_transformed.A.T)\n",
    "    tfidf_vectdf['term'] = tfidf_vect.get_feature_names()\n",
    "    return tfidf_vectdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf(txt.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf(txt.text.values, stop_words=stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "from nltk import word_tokenize\n",
    "\n",
    "exmpl = word_tokenize(\"The quick brown fox jumps over the lazy dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bigrams(exmpl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(trigrams(exmpl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "Shortcoming of Bag of Words method\n",
    "\n",
    "  - It ignores the order of the word, for example, this is bad = bad is this.\n",
    "  - It ignores the context of words. Suppose If I write the sentence \"He loved books. Education is best found in books\". It would create two vectors one for \"He loved books\" and other for \"Education is best found in books.\" It would treat both of them orthogonal which makes them independent, but in reality, they are related to each other \n",
    "\n",
    "To overcome these limitation word embedding was developed and word2vec is an approach to implement such. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import abc\n",
    "\n",
    "model= Word2Vec(abc.sents())\n",
    "X= list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match('See you later, thanks for visiting'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('man','woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "\n",
    "Open class words | Closed class words | Other\n",
    "--- | --- | ---\n",
    "ADJ | ADP | PUNCT\n",
    "ADV | AUX | SYM\n",
    "INTJ | CCONJ | X\n",
    "NOUN | DET |\t \n",
    "PROPN | NUM |\n",
    "VERB |PART |\t \n",
    "PRON |  \n",
    "SCONJ |\n",
    "\n",
    "#### VERB: verb\n",
    "A verb is a member of the syntactic class of words that typically signal events and actions, can constitute a minimal predicate in a clause, and govern the number and types of other constituents which may occur in the clause. Verbs are often associated with grammatical categories like tense, mood, aspect and voice, which can either be expressed inflectionally or using auxilliary verbs or particles.\n",
    "\n",
    "<p style=\"text-align: center;\">What is the verb in this sentence?</p>\n",
    "\n",
    "\n",
    "#### AUX: auxiliary\n",
    "An auxiliary is a function word that accompanies the lexical verb of a verb phrase and expresses grammatical distinctions not carried by the lexical verb, such as person, number, tense, mood, aspect, voice or evidentiality. It is often a verb (which may have non-auxiliary uses as well) but many languages have nonverbal TAME markers and these should also be tagged AUX. The class AUX also include copulas (in the narrow sense of pure linking words for nonverbal predication).\n",
    "\n",
    "Examples\n",
    "   - Tense auxiliaries: *has* (done), is (doing), will (do)\n",
    "   - Passive auxiliaries: *was* (done), *got* (done)\n",
    "   - Modal auxiliaries: *should* (do), *must* (do)\n",
    "   - Verbal copulas: He *is* a teacher.\n",
    "\n",
    "#### NOUN: noun\n",
    "Nouns are a part of speech typically denoting a person, place, thing, animal or idea.\n",
    "\n",
    "The NOUN tag is intended for common nouns only. See PROPN for proper nouns and PRON for pronouns.\n",
    "\n",
    "Examples\n",
    "  - girl\n",
    "  - cat\n",
    "  - tree\n",
    "  \n",
    "#### PROPN: proper noun  \n",
    "A proper noun is a noun (or nominal content word) that is the name (or part of the name) of a specific individual, place, or object.\n",
    "\n",
    "Acronyms of proper nouns, such as UN and NATO, should be tagged PROPN.\n",
    "\n",
    "#### ADJ: adjective\n",
    "Adjectives are words that typically modify nouns and specify their properties or attributes:\n",
    " - The car is *green*.\n",
    "\n",
    "Numbers vs. Adjectives: In general, cardinal numbers receive the part of speech NUM, while ordinal numbers (more precisely adjectival ordinal numerals) receive the tag ADJ.\n",
    "\n",
    "Examples:\n",
    "  - big\n",
    "  - old\n",
    "  - green\n",
    "  - African\n",
    "  - incomprehensible\n",
    "  - first, second, third\n",
    "\n",
    "#### ADP: adposition\n",
    "Adposition is a cover term for prepositions and postpositions. Adpositions belong to a closed set of items that occur before (preposition) or after (postposition) a complement composed of a noun phrase, noun, pronoun, or clause that functions as a noun phrase, and that form a single structure with the complement to express its grammatical and semantic relation to another unit within a clause.\n",
    "\n",
    "Examples\n",
    "  - in\n",
    "  - to\n",
    "  - during\n",
    "\n",
    "#### ADV: adverb\n",
    "Adverbs are words that typically modify verbs for such categories as time, place, direction or manner. They may also modify adjectives and other adverbs, as in *very briefly* or *arguably wrong*.\n",
    "\n",
    "Examples\n",
    "   - very\n",
    "   - well\n",
    "   - exactly\n",
    "   - tomorrow\n",
    "   - up, down\n",
    "   - interrogative adverbs: where, when, how, why\n",
    "   - demonstrative adverbs: here, there, now, then\n",
    "   - indefinite adverbs: somewhere, sometime, anywhere, anytime\n",
    "   - totality adverbs: everywhere, always\n",
    "   - negative adverbs: nowhere, never\n",
    "\n",
    "#### NUM: numeral\n",
    "A numeral is a word, functioning most typically as a determiner, adjective or pronoun, that expresses a number and a relation to the number, such as quantity, sequence, frequency or fraction.\n",
    "\n",
    "#### DET: determiner\n",
    "Determiners are words that modify nouns or noun phrases and express the reference of the noun phrase in context. That is, a determiner may indicate whether the noun is referring to a definite or indefinite element of a class, to a closer or more distant element, to an element belonging to a specified person or thing, to a particular number or quantity, etc.\n",
    "\n",
    "Examples\n",
    "   - articles (a closed class indicating definiteness, specificity or givenness): a, an, the\n",
    "   - possessive determiners (which modify a nominal): my, your\n",
    "   - demonstrative determiners: *this* as in I saw *this* car yesterday.\n",
    "   - interrogative determiners: *which* as in \"*Which* car do you like?\"\n",
    "   - relative determiners: *which* as in \"I wonder *which* car you like.\"\n",
    "   - quantity determiners (quantifiers): indefinite *any*, universal: *all*, and negative *no* as in \"We have *no* cars available.”\n",
    "\n",
    "#### PRON: pronoun\n",
    "Pronouns are words that substitute for nouns or noun phrases, whose meaning is recoverable from the linguistic or extralinguistic context.\n",
    "See also general principles on pronominal words for more tips on how to define pronouns. In particular:\n",
    "  - Non-possessive personal, reflexive or reciprocal pronouns are always tagged PRON.\n",
    "  - Possessives vary across languages. In some languages the above tests put them in the DET category. In others, they are more like a normal personal pronoun in a specific case (often the genitive), or a personal pronoun with an adposition; they are tagged PRON.\n",
    "\n",
    "#### INTJ: interjection\n",
    "An interjection is a word that is used most often as an exclamation or part of an exclamation. It typically expresses an emotional reaction, is not syntactically related to other accompanying expressions, and may include a combination of sounds not otherwise found in the language.\n",
    "\n",
    "Examples\n",
    "   - psst\n",
    "   - ouch\n",
    "   - bravo\n",
    "   - hello\n",
    "   \n",
    "#### CCONJ: coordinating conjunction\n",
    "A coordinating conjunction is a word that links words or larger constituents without syntactically subordinating one to the other and expresses a semantic relationship between them.\n",
    "\n",
    "Examples\n",
    "   - and\n",
    "   - or\n",
    "   - but\n",
    "\n",
    "#### SCONJ: subordinating conjunction\n",
    "A subordinating conjunction is a conjunction that links constructions by making one of them a constituent of the other. The subordinating conjunction typically marks the incorporated constituent which has the status of a (subordinate) clause.\n",
    "\n",
    "Examples\n",
    "    - *that* as in I believe *that* he will come.\n",
    "    - if\n",
    "    - while\n",
    "\n",
    "#### SYM: symbol\n",
    "A symbol is a word-like entity that differs from ordinary words by form, function, or both.\n",
    "\n",
    "Many symbols are or contain special non-alphanumeric characters, similarly to punctuation. What makes them different from punctuation is that they can be substituted by normal words. This involves all currency symbols, e.g. $ 75 is identical to seventy-five dollars.\n",
    "\n",
    "#### PUNCT: punctuation\n",
    "Punctuation marks are non-alphabetical characters and character groups used in many languages to delimit linguistic units in printed text.\n",
    "\n",
    "Punctuation is not taken to include logograms such as $, \\%, and §, which are instead tagged as SYM.\n",
    "\n",
    "#### X: other\n",
    "The tag X is used for words that for some reason cannot be assigned a real part-of-speech category. It should be used very restrictively.\n",
    "\n",
    "\n",
    "[Source](https://universaldependencies.org/u/pos/), note it has the above definition on many different languages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "print(txt.text.values[0])\n",
    "pos_tag(word_tokenize(txt.text.values[0]), tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple example of text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, num_label in zip(news.data[:10], news.target[:10]):\n",
    "    print('[%s]:\\t\\t \"%s ...\"' % (news.target_names[num_label], text[:100].split('\\n')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train(classifier, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=33)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(\"Accuracy: %s\" % classifier.score(X_test, y_test))\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "trial1 = Pipeline(\n",
    "    [('vectorizer', TfidfVectorizer()),\n",
    "     ('classifier', MultinomialNB()),])\n",
    "train(trial1, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "trial2 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'))),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    "train(trial2, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial3 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'),\n",
    "min_df=5)),\n",
    "    ('classifier', MultinomialNB(alpha=0.05)),\n",
    "])\n",
    "train(trial3, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def stemming_tokenizer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in word_tokenize(text)]\n",
    "\n",
    "trial4 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(tokenizer=stemming_tokenizer, stop_words=stopwords.words('english') + list(string.punctuation))),\n",
    "    ('classifier', MultinomialNB(alpha=0.05)),\n",
    "])\n",
    "train(trial4, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional resources\n",
    "\n",
    "### LDA\n",
    "A good visualization tool for LDA topics and keywords is LDAVis in R that has been ported to python [pyLDAvis](https://github.com/bmabey/pyLDAvis).\n",
    "\n",
    "[A good notebook on LDA](http://nbviewer.jupyter.org/github/bmabey/hacker_news_topic_modelling/blob/master/HN%20Topic%20Model%20Talk.ipynb).\n",
    "\n",
    "### PubMed artcile classification\n",
    "A [model](https://github.com/melcutz/NLU_tutorial/blob/master/3_spacy_pubmed_model.ipynb) that is able to tell if a Pubmed article is refering to child or adult patient(s).\n",
    "\n",
    "### POS tagging\n",
    "[Negation detection](https://github.com/melcutz/NLP-demo-2017/blob/master/SpaCy_Intro.ipynb) using POS tagging and syntactic dependencies.\n",
    "\n",
    "### Biopython\n",
    "[Biopython](http://biopython.org/DIST/docs/tutorial/Tutorial.html) has a vast amount of tutorial on biomedical data processing using python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
