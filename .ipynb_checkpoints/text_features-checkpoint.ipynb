{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a classification only using textual features in order to see their potential alone.\n",
    "# as textual features, tfidfs are used and we apply dimensionality reduction later.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "    \n",
    "data_train = pd.read_csv('task1/data/diab_train.csv', index_col=0)\n",
    "data_test = pd.read_csv('task1/data/diab_test.csv', index_col=0)\n",
    "data_validation = pd.read_csv('task1/data/diab_validation.csv', index_col=0)\n",
    "data = pd.concat([data_train, data_validation, data_test], axis=0)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def extract_textual_features(data, colname):\n",
    "    corpus = data[colname]\n",
    "    corpus = corpus.replace(np.NaN, '').values\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    text_features = pd.DataFrame.sparse.from_spmatrix(X)\n",
    "    return text_features\n",
    "\n",
    "# names = ['diag_1_desc', 'diag_2_desc', 'diag_3_desc']\n",
    "# data_original = data.copy()\n",
    "# for col in names:\n",
    "#     features = extract_textual_features(data_original, col)\n",
    "#     data = pd.concat([data, features], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and this', 'document is', 'first document', 'is the', 'is this', 'second document', 'the first', 'the second', 'the third', 'third one', 'this document', 'this is', 'this the']\n",
      "(4, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.52303503, 0.42344193, 0.        ,\n",
       "        0.        , 0.52303503, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.52303503, 0.        ],\n",
       "       [0.        , 0.47633035, 0.        , 0.30403549, 0.        ,\n",
       "        0.47633035, 0.        , 0.47633035, 0.        , 0.        ,\n",
       "        0.47633035, 0.        , 0.        ],\n",
       "       [0.49819711, 0.        , 0.        , 0.31799276, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.49819711, 0.49819711,\n",
       "        0.        , 0.39278432, 0.        ],\n",
       "       [0.        , 0.        , 0.43779123, 0.        , 0.55528266,\n",
       "        0.        , 0.43779123, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.55528266]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    "]\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2))\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Acute pericarditis in diseases classified else...\n",
       "1                         Malignant essential hypertension\n",
       "2              Urinary tract infection, site not specified\n",
       "3                     Respiratory abnormality, unspecified\n",
       "4        Coronary atherosclerosis of unspecified type o...\n",
       "                               ...                        \n",
       "29995                            Pure hypercholesterolemia\n",
       "29996    Diabetes mellitus without mention of complicat...\n",
       "29997                                 Obesity, unspecified\n",
       "29998                                                  NaN\n",
       "29999    Diabetes mellitus without mention of complicat...\n",
       "Length: 30000, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply tfidf to whole data together, and then for each patient take the mean of all 3 diagnoses\n",
    "\n",
    "\n",
    "textdata = pd.concat([data['diag_1_desc'], data['diag_2_desc'], data['diag_3_desc']], axis=0)\n",
    "textdata.reset_index(inplace=True, drop=True)\n",
    "textdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "textdata = textdata.replace(np.NaN, '').values\n",
    "# vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5)\n",
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.5)\n",
    "X = vectorizer.fit_transform(textdata)\n",
    "text_features = pd.DataFrame.sparse.from_spmatrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = X.reshape(10000,-1)\n",
    "text_features.shape\n",
    "type(text_features)\n",
    "text_features = text_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1077) (10000, 1077) (10000, 1077)\n"
     ]
    }
   ],
   "source": [
    "diag1 = text_features[:,:1077]\n",
    "diag2 = text_features[:,1077:2154]\n",
    "diag3 = text_features[:, 2154:]\n",
    "\n",
    "print(diag1.shape, diag2.shape, diag3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 200)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text_mean = diag1 + diag2 + diag3 \n",
    "text_mean.shape\n",
    "\n",
    "#### we can either leave the features as they are. i.e 1077 features or we take the ones that have the highest \n",
    "#### variance among all columns #####\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "transformer = KernelPCA(n_components=200, kernel='rbf', random_state=10)\n",
    "text_mean = transformer.fit_transform(text_mean)\n",
    "text_mean.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting...\n",
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  16 out of  16 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score:  0.52925\n",
      "best parameters:  {'C': 100, 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Accuracy: 0.528 AUCROC: 0.5062210664774941 F1: 0.40253164556962023\n"
     ]
    }
   ],
   "source": [
    "# now do SVM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, auc, roc_curve, make_scorer, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import auxilary\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = text_mean[:8000,:]\n",
    "y_train = data['readmitted'].values[:8000]\n",
    "X_test = text_mean[8000:,:]\n",
    "y_test = data['readmitted'].values[8000:]\n",
    "\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "scoring ={'auroc':    make_scorer(roc_auc_score, greater_is_better=True),\n",
    "        'f1_score': make_scorer(f1_score, average='micro', greater_is_better=True)}\n",
    "\n",
    "\n",
    "parameters = {'kernel': ['rbf'],\n",
    "              'C': [0.1, 1, 10, 100],\n",
    "              'gamma': ['scale'],\n",
    "              'degree': [3]}\n",
    "\n",
    "\n",
    "classifier = SVC(random_state=423, class_weight='balanced', max_iter=80000)\n",
    "grid_classifier = GridSearchCV(classifier, parameters, cv=4, verbose=3, scoring=scoring, n_jobs=3, refit='f1_score')\n",
    "print('fitting...')\n",
    "grid_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_predict = grid_classifier.predict(X_test)\n",
    "print('best score: ', grid_classifier.best_score_)\n",
    "print('best parameters: ', grid_classifier.best_params_)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "aucroc_score = roc_auc_score(y_test, y_predict)\n",
    "f1Score = f1_score(y_test, y_predict)\n",
    "print('Accuracy: {} AUCROC: {} F1: {}'.format(accuracy, aucroc_score, f1Score))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
