{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-82a1b4c80e23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdependency\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhard_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdependency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mmissing_dependencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{dependency}: {e}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytz\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpytz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLazyList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLazySet\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpytz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtzinfo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseTzInfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpytz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtzfile\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuild_tzinfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif, chi2 \n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    data_original = data.copy()\n",
    "    data = data.replace('?', np.NaN)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    # weights, payer_code, diag_1_desc, diag_2_desc, diag_3_desc\n",
    "    data.drop(labels=['weight', 'payer_code', 'diag_1_desc', 'diag_2_desc', 'diag_3_desc'], axis=1, inplace=True)\n",
    "\n",
    "    data['diag_1'] = group_diagnoses(data['diag_1'])\n",
    "    data['diag_2'] = group_diagnoses(data['diag_2'])\n",
    "    data['diag_3'] = group_diagnoses(data['diag_3'])\n",
    "\n",
    "    # Encode string data to numericals\n",
    "    to_cat = list(data.select_dtypes(['object']).columns)\n",
    "    data[to_cat] = data[to_cat].astype('category')\n",
    "    cat_columns = data.select_dtypes(['category']).columns\n",
    "    data[cat_columns] = data[cat_columns].apply(lambda x: x.cat.codes)\n",
    "\n",
    "    # Get Readmitted as labels\n",
    "    labels = data['readmitted']\n",
    "    data.drop(labels=['readmitted'], axis=1, inplace=True)\n",
    "    # data = data.replace(-1, np.NaN)\n",
    "\n",
    "\n",
    "    return labels.values.ravel(), data.values\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(text) \n",
    "    text = [w for w in word_tokens if not w in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return WordNetLemmatizer().lemmatize(text)\n",
    "\n",
    "\n",
    "def extract_textual_features(data, colname):\n",
    "    corpus = data[colname]\n",
    "    corpus = corpus.replace(np.NaN, '').values\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5, min_df=0.0001)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    text_features = pd.DataFrame.sparse.from_spmatrix(X)\n",
    "    return text_features\n",
    "\n",
    "\n",
    "def group_diagnoses(df):\n",
    "    # Create mapping from\n",
    "    l_old = []\n",
    "    l_new = []\n",
    "\n",
    "    idx = 0\n",
    "    tmp_list1 = list(range(390, 460))\n",
    "    tmp_list1 += [785]\n",
    "    tmp_list2 = [idx] * len(tmp_list1)\n",
    "    idx += 1\n",
    "\n",
    "    l_old = [*l_old, *tmp_list1]\n",
    "    l_new = [*l_new, *tmp_list2]\n",
    "\n",
    "    tmp_list1 = list(range(460, 520))\n",
    "    tmp_list1 += [786]\n",
    "    tmp_list2 = [idx] * len(tmp_list1)\n",
    "    idx += 1\n",
    "\n",
    "    l_old = [*l_old, *tmp_list1]\n",
    "    l_new = [*l_new, *tmp_list2]\n",
    "\n",
    "    tmp_list1 = list(range(520, 579))\n",
    "    tmp_list1 += [787]\n",
    "    tmp_list2 = [idx] * len(tmp_list1)\n",
    "    idx += 1\n",
    "\n",
    "    l_old = [*l_old, *tmp_list1]\n",
    "    l_new = [*l_new, *tmp_list2]\n",
    "\n",
    "    tmp_list1 = [str(i) for i in list(np.arange(250, 251, 0.01))]\n",
    "    tmp_list2 = [idx] * len(tmp_list1)\n",
    "    idx += 1\n",
    "    l_old = [*l_old, *tmp_list1]\n",
    "    l_new = [*l_new, *tmp_list2]\n",
    "\n",
    "    tmp_list1 = range(800, 1000)\n",
    "    tmp_list2 = [idx] * len(tmp_list1)\n",
    "    idx += 1\n",
    "    l_old = [*l_old, *tmp_list1]\n",
    "    l_new = [*l_new, *tmp_list2]\n",
    "\n",
    "    tmp_list1 = range(710, 740)\n",
    "    tmp_list2 = [idx] * len(tmp_list1)\n",
    "    idx += 1\n",
    "    l_old = [*l_old, *tmp_list1]\n",
    "    l_new = [*l_new, *tmp_list2]\n",
    "\n",
    "    tmp_list1 = list(range(580, 630))\n",
    "    tmp_list1 += [788]\n",
    "    tmp_list2 = [idx] * len(tmp_list1)\n",
    "    idx += 1\n",
    "    l_old = [*l_old, *tmp_list1]\n",
    "    l_new = [*l_new, *tmp_list2]\n",
    "\n",
    "    tmp_list1 = range(140, 240)\n",
    "    tmp_list2 = [idx] * len(tmp_list1)\n",
    "    idx += 1\n",
    "    l_old = [*l_old, *tmp_list1]\n",
    "    l_new = [*l_new, *tmp_list2]\n",
    "\n",
    "    l_old = [str(i) for i in l_old]\n",
    "    d = dict(zip(l_old, l_new))\n",
    "\n",
    "    df_new = df.copy()\n",
    "\n",
    "    df_new = df_new.map(d)\n",
    "    df_new = df_new.replace(df_new[pd.isna(df_new)], 8)\n",
    "    df_new = df_new.astype(int)\n",
    "    return df_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('task1/data/diab_train.csv', index_col=0)\n",
    "data_test = pd.read_csv('task1/data/diab_test.csv', index_col=0)\n",
    "data_validation = pd.read_csv('task1/data/diab_validation.csv', index_col=0)\n",
    "\n",
    "data_train = pd.concat([data_train, data_validation, data_test], axis=0)\n",
    "data_train.reset_index(drop=True, inplace=True)\n",
    "data = data_train.copy()\n",
    "y, X = preprocess_data(data_train)\n",
    "print(y.shape, X.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a function for xgboost training.\n",
    "#### We will call this function later for different types of textual feature extraction\n",
    "\n",
    "def train_xgboost(X, y):\n",
    "    X_train = X[:8000]\n",
    "    y_train = y[:8000]\n",
    "    y_test, X_test = y[8000:], X[8000:]\n",
    "    print('X_train shape: {}  and X_test shape: {}'.format(X_train.shape, X_test.shape))\n",
    "\n",
    "    scoring ={'auroc':    make_scorer(roc_auc_score, greater_is_better=True),\n",
    "            'f1_score': make_scorer(f1_score, average='micro', greater_is_better=True)}\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(scale_pos_weight=2, disable_defeult_eval_metric=0)\n",
    "\n",
    "    parameters = {\n",
    "        'objective': ['binary:logistic'],\n",
    "        'max_depth': [200],\n",
    "        'min_child_weight': [9],\n",
    "        'n_estimators': [500],\n",
    "        'seed': [11],\n",
    "        'learning_rate': [0.01],\n",
    "        'max_delta_step': [0],\n",
    "        'subsample': [0.75]\n",
    "    }\n",
    "\n",
    "    clf = GridSearchCV(estimator=xgb_model, param_grid=parameters, n_jobs=3, cv=4, scoring=scoring, verbose=3,\n",
    "                       refit='f1_score')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print('best score: ', clf.best_score_)\n",
    "    print('best parameters: ', clf.best_params_)\n",
    "\n",
    "    y_predict = clf.predict(X_test)\n",
    "    print(\"0 predictions: {}  1 predictions: {}\".format(np.count_nonzero(y_predict == 0) ,np.count_nonzero(y_predict == 1)))\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    aucroc_score = roc_auc_score(y_test, y_predict)\n",
    "    f1Score = f1_score(y_test, y_predict)\n",
    "    print('Accuracy: {} AUCROC: {} F1: {}'.format(accuracy, aucroc_score, f1Score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### There are 2 options: Either you can train all the model with features extracted only from primary diagnoses\n",
    "##### or you can train all the models with features extracted from all 3 description of diagnoses\n",
    "##### Change the OPTION1 variable appropriately. If you want features from all, set it to False\n",
    "\n",
    "OPTION1 = False\n",
    "\n",
    "if OPTION1:\n",
    "    textdata = data['diag_1_desc']\n",
    "    textdata = diag1.replace(np.NaN, '').values\n",
    "else:\n",
    "    textdata = pd.concat([data['diag_1_desc'], data['diag_2_desc'], data['diag_3_desc']], axis=0)\n",
    "    textdata.reset_index(inplace=True, drop=True)\n",
    "    textdata = textdata.replace(np.NaN, ' ').values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model (without NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgboost(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.5, preprocessor=preprocess_text)\n",
    "text_features = vectorizer.fit_transform(textdata)\n",
    "text_features = pd.DataFrame.sparse.from_spmatrix(text_features).values\n",
    "\n",
    "print(text_features)\n",
    "\n",
    "text_features = text_features.reshape(10000,-1)\n",
    "print(text_features.shape)\n",
    "\n",
    "if OPTION1==False:\n",
    "    dim = int(text_features.shape[1]/3)\n",
    "    print(dim)\n",
    "    diag1 = text_features[:,:dim]\n",
    "    diag2 = text_features[:,dim:dim*2]\n",
    "    diag3 = text_features[:, 2*dim:]\n",
    "    print(diag1.shape, diag2.shape, diag3.shape)\n",
    "\n",
    "    text_features = diag1 + diag2 + diag3 \n",
    "    text_features.shape\n",
    "\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=20)\n",
    "text_features = selector.fit_transform(text_features, y)\n",
    "text_features.shape\n",
    "scaler = StandardScaler()\n",
    "text_features = scaler.fit_transform(text_features)\n",
    "\n",
    "# now add textual features to the categorical features\n",
    "Xnew = np.concatenate((X, text_features), axis=1)\n",
    "print(Xnew.shape)\n",
    "\n",
    "train_xgboost(Xnew, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5, preprocessor=preprocess_text)\n",
    "text_features = vectorizer.fit_transform(textdata)\n",
    "text_features = pd.DataFrame.sparse.from_spmatrix(text_features).values\n",
    "\n",
    "print(text_features)\n",
    "\n",
    "text_features = text_features.reshape(10000,-1)\n",
    "print(text_features.shape)\n",
    "\n",
    "if OPTION1:\n",
    "    dim = int(text_features.shape[1]/3)\n",
    "    print(dim)\n",
    "    diag1 = text_features[:,:dim]\n",
    "    diag2 = text_features[:,dim:dim*2]\n",
    "    diag3 = text_features[:, 2*dim:]\n",
    "    print(diag1.shape, diag2.shape, diag3.shape)\n",
    "\n",
    "    text_features = diag1 + diag2 + diag3 \n",
    "    text_features.shape\n",
    "\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=20)\n",
    "text_features = selector.fit_transform(text_features, y)\n",
    "text_features.shape\n",
    "scaler = StandardScaler()\n",
    "text_features = scaler.fit_transform(text_features)\n",
    "\n",
    "# now add textual features to the categorical features\n",
    "Xnew = np.concatenate((X, text_features), axis=1)\n",
    "print(Xnew.shape)\n",
    "\n",
    "train_xgboost(Xnew, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.5, preprocessor=preprocess_text, ngram_range=(2,2))\n",
    "text_features = vectorizer.fit_transform(textdata)\n",
    "text_features = pd.DataFrame.sparse.from_spmatrix(text_features).values\n",
    "# print(vectorizer.get_feature_names())\n",
    "# print(text_features)\n",
    "\n",
    "text_features = text_features.reshape(10000,-1)\n",
    "# print(text_features.shape)\n",
    "\n",
    "if OPTION1:\n",
    "    dim = int(text_features.shape[1]/3)\n",
    "    diag1 = text_features[:,:dim]\n",
    "    diag2 = text_features[:,dim:dim*2]\n",
    "    diag3 = text_features[:, 2*dim:]\n",
    "    # print(diag1.shape, diag2.shape, diag3.shape)\n",
    "\n",
    "    text_features = diag1 + diag2 + diag3 \n",
    "    text_features.shape\n",
    "\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=20)\n",
    "text_features = selector.fit_transform(text_features, y)\n",
    "text_features.shape\n",
    "scaler = StandardScaler()\n",
    "text_features = scaler.fit_transform(text_features)\n",
    "\n",
    "# now add textual features to the categorical features\n",
    "Xnew = np.concatenate((X, text_features), axis=1)\n",
    "print(Xnew.shape)\n",
    "\n",
    "train_xgboost(Xnew, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.5, preprocessor=preprocess_text, ngram_range=(3,3))\n",
    "text_features = vectorizer.fit_transform(textdata)\n",
    "text_features = pd.DataFrame.sparse.from_spmatrix(text_features).values\n",
    "# print(vectorizer.get_feature_names())\n",
    "# print(text_features)\n",
    "\n",
    "text_features = text_features.reshape(10000,-1)\n",
    "# print(text_features.shape)\n",
    "\n",
    "if OPTION1:\n",
    "    dim = int(text_features.shape[1]/3)\n",
    "    diag1 = text_features[:,:dim]\n",
    "    diag2 = text_features[:,dim:dim*2]\n",
    "    diag3 = text_features[:, 2*dim:]\n",
    "    #   print(diag1.shape, diag2.shape, diag3.shape)\n",
    "\n",
    "    text_features = diag1 + diag2 + diag3 \n",
    "    text_features.shape\n",
    "\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=20)\n",
    "text_features = selector.fit_transform(text_features, y)\n",
    "text_features.shape\n",
    "scaler = StandardScaler()\n",
    "text_features = scaler.fit_transform(text_features)\n",
    "\n",
    "# now add textual features to the categorical features\n",
    "Xnew = np.concatenate((X, text_features), axis=1)\n",
    "print(Xnew.shape)\n",
    "\n",
    "train_xgboost(Xnew, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram as TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5, preprocessor=preprocess_text, ngram_range=(2,2))\n",
    "text_features = vectorizer.fit_transform(textdata)\n",
    "text_features = pd.DataFrame.sparse.from_spmatrix(text_features).values\n",
    "# print(vectorizer.get_feature_names())\n",
    "# print(text_features)\n",
    "\n",
    "text_features = text_features.reshape(10000,-1)\n",
    "print(text_features.shape)\n",
    "\n",
    "if OPTION1:\n",
    "    dim = int(text_features.shape[1]/3)\n",
    "    diag1 = text_features[:,:dim]\n",
    "    diag2 = text_features[:,dim:dim*2]\n",
    "    diag3 = text_features[:, 2*dim:]\n",
    "    # print(diag1.shape, diag2.shape, diag3.shape)\n",
    "\n",
    "    text_features = diag1 + diag2 + diag3 \n",
    "    text_features.shape\n",
    "\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=20)\n",
    "text_features = selector.fit_transform(text_features, y)\n",
    "text_features.shape\n",
    "scaler = StandardScaler()\n",
    "text_features = scaler.fit_transform(text_features)\n",
    "\n",
    "# now add textual features to the categorical features\n",
    "Xnew = np.concatenate((X, text_features), axis=1)\n",
    "print(Xnew.shape)\n",
    "\n",
    "train_xgboost(Xnew, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram as TDIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5, preprocessor=preprocess_text, ngram_range=(3,3))\n",
    "text_features = vectorizer.fit_transform(textdata)\n",
    "text_features = pd.DataFrame.sparse.from_spmatrix(text_features).values\n",
    "# print(vectorizer.get_feature_names())\n",
    "# print(text_features)\n",
    "\n",
    "text_features = text_features.reshape(10000,-1)\n",
    "# print(text_features.shape)\n",
    "\n",
    "if OPTION1:\n",
    "    dim = int(text_features.shape[1]/3)\n",
    "    diag1 = text_features[:,:dim]\n",
    "    diag2 = text_features[:,dim:dim*2]\n",
    "    diag3 = text_features[:, 2*dim:]\n",
    "#     print(diag1.shape, diag2.shape, diag3.shape)\n",
    "\n",
    "    text_features = diag1 + diag2 + diag3 \n",
    "    text_features.shape\n",
    "\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=20)\n",
    "text_features = selector.fit_transform(text_features, y)\n",
    "text_features.shape\n",
    "scaler = StandardScaler()\n",
    "text_features = scaler.fit_transform(text_features)\n",
    "\n",
    "# now add textual features to the categorical features\n",
    "Xnew = np.concatenate((X, text_features), axis=1)\n",
    "print(Xnew.shape)\n",
    "\n",
    "train_xgboost(Xnew, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentences = [word_tokenize(sentence) for sentence in textdata]\n",
    "model= Word2Vec(sentences, min_count=1, size=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(sentence):\n",
    "    total = 0\n",
    "    for word in sentence:\n",
    "        total += model.wv[word]\n",
    "        \n",
    "    if len(sentence) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return total/len(sentence)\n",
    "\n",
    "text_features = [compute_features(sentence) for sentence in sentences]\n",
    "text_features = np.array(text_features)\n",
    "text_features.shape\n",
    "text_features = text_features.reshape(10000,-1)\n",
    "text_features = np.sum(text_features, axis=1)\n",
    "text_features = np.stack(text_features)\n",
    "text_features.shape\n",
    "\n",
    "scaler = StandardScaler()\n",
    "text_features = scaler.fit_transform(text_features)\n",
    "\n",
    "Xnew = np.concatenate((X, text_features), axis=1)\n",
    "\n",
    "train_xgboost(Xnew, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
